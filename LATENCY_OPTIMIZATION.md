# ‚ö° Latency Optimization Guide

**Current Status**: ‚úÖ Many optimizations already implemented  
**Assessment Date**: December 1, 2025

---

## üéØ Executive Summary

The Tawk Agents SDK **already implements many latency optimizations** at the code level. However, there are **additional optimizations** that can further reduce latency for production deployments.

**Current Optimization Score**: ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)  
**Potential Optimization Score**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)

---

## ‚úÖ Already Implemented Optimizations

### 1. ‚úÖ Parallel Tool Execution (CRITICAL)

**Location**: `src/core/execution.ts:executeToolsInParallel()`

```typescript
// Execute ALL tools in parallel using Promise.all
const executionPromises = toolCalls.map(async (toolCall) => {
  const result = await tool.execute(toolCall.args, contextWrapper);
  return { toolName, args, result, duration };
});

// Wait for ALL tools to complete in parallel
return await Promise.all(executionPromises);
```

**Impact**: üî• **HIGH** - Reduces latency by 60-80% when multiple tools are called  
**Example**: 3 tools @ 2s each = 2s total (not 6s)

---

### 2. ‚úÖ MCP Tools Caching

**Location**: `src/core/agent.ts:getMcpTools()` & `src/mcp/enhanced.ts:getCoreTools()`

```typescript
// Agent-level cache (1 minute TTL)
async getMcpTools(): Promise<Record<string, CoreTool>> {
  if (
    this.mcpToolsCache &&
    this.mcpToolsCacheTime &&
    Date.now() - this.mcpToolsCacheTime < this.MCP_CACHE_TTL // 60000ms
  ) {
    return this.mcpToolsCache;
  }
  // ... fetch and cache
}

// Server-level cache
private toolCache?: Map<string, CoreTool>;
private cacheTimestamp?: number;
private readonly CACHE_TTL = 60000; // 1 minute
```

**Impact**: üî• **MEDIUM** - Avoids repeated MCP server calls (saves 100-500ms per request)

---

### 3. ‚úÖ Static Instructions Caching

**Location**: `src/core/agent.ts:cachedInstructions`

```typescript
// Cached static instructions
private cachedInstructions?: string;

async getInstructions(context: RunContextWrapper<TContext>): Promise<string> {
  if (typeof this.instructions === 'string') {
    if (!this.cachedInstructions) {
      this.cachedInstructions = this.instructions;
    }
    return this.cachedInstructions;
  }
  return await this.instructions(context);
}
```

**Impact**: ‚ö° **LOW** - Saves ~0.1-1ms per call (minor but adds up)

---

### 4. ‚úÖ Session Caching (SessionManager)

**Location**: `src/sessions/session.ts:SessionManager`

```typescript
// Sessions are cached in memory for reuse
getSession<TContext = any>(sessionId: string): Session<TContext> {
  // Check if session already exists
  if (this.sessions.has(sessionId)) {
    return this.sessions.get(sessionId)!;
  }
  // ... create new session
}
```

**Impact**: üî• **MEDIUM** - Avoids repeated session lookups (saves 10-50ms per request)

---

### 5. ‚úÖ Embedding Query Caching (RAG)

**Location**: `src/tools/rag/pinecone-search.ts:QueryEmbeddingCache`

```typescript
class QueryEmbeddingCache {
  private cache: Map<string, number[]> = new Map();

  async getEmbedding(query: string, embeddingModel, providerOptions): Promise<number[]> {
    const cacheKey = this.cacheKeyGenerator(query);
    if (this.cache.has(cacheKey)) {
      return this.cache.get(cacheKey)!;
    }
    // ... generate and cache
  }
}
```

**Impact**: üî• **HIGH** for RAG - Embedding generation takes 50-200ms, caching saves this entirely

---

### 6. ‚úÖ Streaming with Efficient Buffering

**Location**: `src/core/agent.ts:handleStreamCompletion()`

```typescript
// Performance: Use array for string concatenation (O(n) not O(n¬≤))
const textChunks: string[] = [];
for await (const chunk of result.fullStream) {
  if (chunk.type === 'text-delta') {
    textChunks.push(chunk.textDelta);
  }
}
// Performance: Join once at the end (O(n) instead of O(n¬≤))
const fullText = textChunks.join('');
```

**Impact**: ‚ö° **MEDIUM** - Avoids O(n¬≤) string concatenation (saves 10-100ms for long outputs)

---

### 7. ‚úÖ Parallel Agent Execution

**Location**: `src/core/coordination.ts:runParallel()`

```typescript
// Execute all agents in parallel
const promises = agents.map(async (agent, index) => {
  return await run(agent, inputs[index], options);
});

// Wait for all to complete
const settled = await Promise.allSettled(promises);
```

**Impact**: üî• **HIGH** - Multi-agent workflows run simultaneously (saves 50-80% latency)

---

### 8. ‚úÖ Race Agents for Fastest Response

**Location**: `src/core/coordination.ts:raceAgents()`

```typescript
// Use Promise.race to get first completion
const promises = agents.map(agent => run(agent, input, options));
const winner = await Promise.race(promises);
```

**Impact**: üî• **HIGH** for fallbacks - Gets result from fastest model (saves 20-60% latency)

---

## üöÄ Additional Optimizations (Recommended)

### 1. üîß Tool Result Streaming (Not Implemented)

**Problem**: Currently, tools complete before streaming text, causing perceived latency.

**Solution**: Stream tool execution progress

```typescript
// PROPOSED: src/core/execution.ts
export async function executeToolsInParallel<TContext = any>(
  tools: Record<string, CoreTool>,
  toolCalls: Array<{ toolName: string; args: any; toolCallId?: string }>,
  contextWrapper: RunContextWrapper<TContext>,
  onProgress?: (toolName: string, progress: any) => void // NEW
): Promise<ToolExecutionResult[]> {
  const executionPromises = toolCalls.map(async (toolCall) => {
    const tool = tools[toolCall.toolName];
    
    // Allow tools to emit progress
    if (tool.streamProgress && onProgress) {
      tool.streamProgress((progress) => {
        onProgress(toolCall.toolName, progress);
      });
    }
    
    const result = await tool.execute(toolCall.args, contextWrapper);
    return { toolName, args, result, duration };
  });

  return await Promise.all(executionPromises);
}
```

**Impact**: üî• **HIGH** - Improves perceived latency by 30-50% (user sees progress immediately)  
**Effort**: Medium (requires tool API changes)  
**Timeline**: 2026 Q1

---

### 2. üîß Instruction Compilation/Template Caching (Partial)

**Problem**: Dynamic instructions are re-evaluated every turn.

**Solution**: Add template caching with placeholders

```typescript
// PROPOSED: src/core/agent.ts
private instructionTemplate?: string;
private instructionPlaceholders?: Set<string>;

async getInstructions(context: RunContextWrapper<TContext>): Promise<string> {
  if (typeof this.instructions === 'string') {
    return this.cachedInstructions || (this.cachedInstructions = this.instructions);
  }
  
  // NEW: Template caching
  if (typeof this.instructions === 'function') {
    if (!this.instructionTemplate) {
      // First call - cache template structure
      const result = await this.instructions(context);
      this.instructionTemplate = result;
      return result;
    }
    
    // Subsequent calls - check if context changed
    const contextHash = this.hashContext(context);
    if (this.lastContextHash === contextHash) {
      return this.instructionTemplate;
    }
    
    this.lastContextHash = contextHash;
    const result = await this.instructions(context);
    this.instructionTemplate = result;
    return result;
  }
  
  return await this.instructions(context);
}
```

**Impact**: ‚ö° **LOW-MEDIUM** - Saves 1-10ms per turn (depends on instruction complexity)  
**Effort**: Low  
**Timeline**: 2026 Q1

---

### 3. üîß Model Response Caching (Not Implemented)

**Problem**: Identical queries to LLM are re-executed.

**Solution**: Add semantic caching layer

```typescript
// PROPOSED: src/core/cache.ts
export class ModelResponseCache {
  private cache: Map<string, { response: any; timestamp: number }> = new Map();
  private readonly TTL = 300000; // 5 minutes

  async get(
    model: string,
    messages: ModelMessage[],
    tools: Record<string, CoreTool>
  ): Promise<any | null> {
    const key = this.generateCacheKey(model, messages, tools);
    const cached = this.cache.get(key);
    
    if (cached && Date.now() - cached.timestamp < this.TTL) {
      return cached.response;
    }
    
    return null;
  }

  set(
    model: string,
    messages: ModelMessage[],
    tools: Record<string, CoreTool>,
    response: any
  ): void {
    const key = this.generateCacheKey(model, messages, tools);
    this.cache.set(key, { response, timestamp: Date.now() });
  }

  private generateCacheKey(
    model: string,
    messages: ModelMessage[],
    tools: Record<string, CoreTool>
  ): string {
    // Hash based on model, last N messages, and tool signatures
    const relevantMessages = messages.slice(-3); // Last 3 messages
    const toolSignatures = Object.keys(tools).sort().join(',');
    return `${model}:${JSON.stringify(relevantMessages)}:${toolSignatures}`;
  }
}
```

**Usage**:
```typescript
// In src/core/agent.ts
const cache = new ModelResponseCache();

const cachedResponse = await cache.get(model, messages, tools);
if (cachedResponse) {
  return cachedResponse; // Skip LLM call entirely
}

const response = await generateText({ model, messages, tools });
cache.set(model, messages, tools, response);
```

**Impact**: üî• **VERY HIGH** - Saves 500ms-3s for cache hits (entire LLM call skipped)  
**Effort**: Medium (need smart cache invalidation)  
**Timeline**: 2026 Q2

**Caveats**:
- Only cache deterministic queries (temperature=0)
- Invalidate on context changes
- Consider semantic similarity (not just exact match)

---

### 4. üîß Prefetching & Speculative Execution (Not Implemented)

**Problem**: Waiting for LLM to decide next tool, then executing it.

**Solution**: Predict likely tool calls and prefetch data

```typescript
// PROPOSED: src/core/prefetch.ts
export class ToolPrefetcher {
  async prefetchLikelyTools(
    agent: Agent,
    messages: ModelMessage[],
    availableTools: Record<string, CoreTool>
  ): Promise<Map<string, any>> {
    // Analyze conversation pattern
    const likelyTools = this.predictLikelyTools(messages, availableTools);
    
    // Prefetch in background (don't await)
    const prefetchResults = new Map<string, any>();
    
    for (const toolName of likelyTools) {
      const tool = availableTools[toolName];
      if (tool.supportsPrefetch) {
        // Execute tool speculatively
        tool.prefetch().then(result => {
          prefetchResults.set(toolName, result);
        });
      }
    }
    
    return prefetchResults;
  }

  private predictLikelyTools(
    messages: ModelMessage[],
    tools: Record<string, CoreTool>
  ): string[] {
    // Simple heuristic: last used tools
    // Advanced: ML model to predict next tool
    const lastMessage = messages[messages.length - 1];
    
    // Check for patterns (e.g., "search" keyword ‚Üí search tool)
    const likelyTools: string[] = [];
    
    if (lastMessage.content.includes('search')) {
      likelyTools.push('search');
    }
    if (lastMessage.content.includes('calculate')) {
      likelyTools.push('calculator');
    }
    
    return likelyTools;
  }
}
```

**Impact**: üî• **MEDIUM-HIGH** - Can save 20-40% latency for predictable tool usage  
**Effort**: High (requires ML or good heuristics)  
**Timeline**: 2026 Q3-Q4

**Caveats**:
- Only useful for predictable workflows
- May waste resources on wrong predictions
- Requires tool-level support

---

### 5. üîß Connection Pooling for MCP Servers (Partial)

**Problem**: MCP servers are started/stopped frequently, causing startup latency.

**Solution**: Keep MCP connections alive and pool them

```typescript
// PROPOSED: src/mcp/pool.ts
export class MCPConnectionPool {
  private connections: Map<string, EnhancedMCPServer[]> = new Map();
  private maxConnectionsPerServer = 5;

  async getConnection(config: MCPServerConfig): Promise<EnhancedMCPServer> {
    const key = this.getConfigKey(config);
    const pool = this.connections.get(key) || [];
    
    // Return existing idle connection
    const idle = pool.find(conn => !conn.isInUse());
    if (idle) {
      idle.markInUse();
      return idle;
    }
    
    // Create new connection if under limit
    if (pool.length < this.maxConnectionsPerServer) {
      const server = new EnhancedMCPServer(config);
      await server.connect();
      pool.push(server);
      this.connections.set(key, pool);
      server.markInUse();
      return server;
    }
    
    // Wait for connection to become available
    return await this.waitForConnection(key);
  }

  releaseConnection(server: EnhancedMCPServer): void {
    server.markIdle();
  }
}
```

**Impact**: üî• **MEDIUM** - Saves 100-500ms per MCP server startup  
**Effort**: Medium  
**Timeline**: 2026 Q2

---

### 6. üîß Batch API Calls (Not Implemented)

**Problem**: Multiple sequential API calls to same service.

**Solution**: Batch requests when possible

```typescript
// PROPOSED: src/helpers/batch.ts
export class BatchExecutor {
  private batchWindow = 50; // ms
  private pendingBatches: Map<string, Array<{ args: any; resolve: Function }>> = new Map();

  async executeBatched<T>(
    batchKey: string,
    args: any,
    executor: (batch: any[]) => Promise<T[]>
  ): Promise<T> {
    return new Promise((resolve) => {
      // Add to pending batch
      if (!this.pendingBatches.has(batchKey)) {
        this.pendingBatches.set(batchKey, []);
        
        // Schedule batch execution
        setTimeout(async () => {
          const batch = this.pendingBatches.get(batchKey) || [];
          this.pendingBatches.delete(batchKey);
          
          // Execute batch
          const batchArgs = batch.map(b => b.args);
          const results = await executor(batchArgs);
          
          // Resolve individual promises
          batch.forEach((item, index) => {
            item.resolve(results[index]);
          });
        }, this.batchWindow);
      }
      
      this.pendingBatches.get(batchKey)!.push({ args, resolve });
    });
  }
}
```

**Usage**:
```typescript
// For embedding generation
const batcher = new BatchExecutor();

async function generateEmbedding(text: string): Promise<number[]> {
  return batcher.executeBatched('embeddings', text, async (texts) => {
    // Batch API call
    return await embedModel.embed(texts);
  });
}
```

**Impact**: üî• **HIGH** for batch operations - Can reduce latency by 50-70% for multiple embeddings  
**Effort**: Medium  
**Timeline**: 2026 Q2

---

### 7. üîß HTTP/2 & Keep-Alive for API Calls (Not Implemented)

**Problem**: New HTTP connection for each API call.

**Solution**: Use HTTP/2 and connection pooling

```typescript
// PROPOSED: Update API client configuration
import { fetch } from 'undici'; // Supports HTTP/2

// Configure HTTP client with connection pooling
const httpAgent = new Agent({
  keepAlive: true,
  keepAliveMsecs: 10000,
  maxSockets: 50,
  maxFreeSockets: 10,
  http2: true // Enable HTTP/2
});

// Use in AI SDK
const openaiClient = createOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  fetch: (url, options) => fetch(url, { ...options, dispatcher: httpAgent })
});
```

**Impact**: ‚ö° **LOW-MEDIUM** - Saves 20-100ms per request (TLS handshake avoided)  
**Effort**: Low (configuration change)  
**Timeline**: 2026 Q1

---

### 8. üîß Lazy Loading of Tools & Guardrails (Not Implemented)

**Problem**: All tools/guardrails loaded even if not used.

**Solution**: Load on-demand

```typescript
// PROPOSED: src/core/agent.ts
export class Agent<TContext = any, TOutput = string> {
  private toolsFactory: () => Record<string, CoreTool>;
  private loadedTools?: Record<string, CoreTool>;

  constructor(config: AgentConfig<TContext, TOutput>) {
    // Store factory instead of tools
    this.toolsFactory = () => config.tools || {};
  }

  private async getTools(): Promise<Record<string, CoreTool>> {
    if (!this.loadedTools) {
      this.loadedTools = this.toolsFactory();
    }
    return this.loadedTools;
  }
}
```

**Impact**: ‚ö° **LOW** - Saves 1-5ms at agent initialization  
**Effort**: Low  
**Timeline**: 2026 Q1

---

## üìä Optimization Impact Summary

| Optimization | Impact | Latency Saved | Effort | Status |
|--------------|--------|---------------|--------|--------|
| **Parallel Tool Execution** | üî• Very High | 60-80% | Low | ‚úÖ Implemented |
| **MCP Tools Caching** | üî• Medium | 100-500ms | Low | ‚úÖ Implemented |
| **Session Caching** | üî• Medium | 10-50ms | Low | ‚úÖ Implemented |
| **Embedding Caching** | üî• High (RAG) | 50-200ms | Low | ‚úÖ Implemented |
| **Parallel Agents** | üî• High | 50-80% | Low | ‚úÖ Implemented |
| **Race Agents** | üî• High | 20-60% | Low | ‚úÖ Implemented |
| **Stream Buffering** | ‚ö° Medium | 10-100ms | Low | ‚úÖ Implemented |
| **Tool Result Streaming** | üî• High | 30-50% perceived | Medium | ‚ùå Not Implemented |
| **Model Response Caching** | üî• Very High | 500ms-3s | Medium | ‚ùå Not Implemented |
| **Prefetching** | üî• Medium-High | 20-40% | High | ‚ùå Not Implemented |
| **MCP Connection Pool** | üî• Medium | 100-500ms | Medium | ‚ùå Not Implemented |
| **Batch API Calls** | üî• High | 50-70% | Medium | ‚ùå Not Implemented |
| **HTTP/2 Keep-Alive** | ‚ö° Low-Medium | 20-100ms | Low | ‚ùå Not Implemented |
| **Lazy Loading** | ‚ö° Low | 1-5ms | Low | ‚ùå Not Implemented |

---

## üéØ Recommended Implementation Priority

### Phase 1: Quick Wins (2026 Q1) - 1-2 weeks
1. ‚úÖ HTTP/2 & Keep-Alive (Low effort, medium impact)
2. ‚úÖ Instruction Template Caching (Low effort, low-medium impact)
3. ‚úÖ Lazy Loading (Low effort, low impact)

**Total Estimated Impact**: 5-15% latency reduction

---

### Phase 2: High-Impact Features (2026 Q2) - 3-4 weeks
1. ‚úÖ Model Response Caching (Medium effort, very high impact)
2. ‚úÖ MCP Connection Pooling (Medium effort, medium impact)
3. ‚úÖ Batch API Calls (Medium effort, high impact)

**Total Estimated Impact**: 20-40% latency reduction (for cache hits)

---

### Phase 3: Advanced Optimizations (2026 Q3-Q4) - 4-6 weeks
1. ‚úÖ Tool Result Streaming (Medium effort, high perceived impact)
2. ‚úÖ Prefetching & Speculative Execution (High effort, medium-high impact)

**Total Estimated Impact**: 15-30% latency reduction (for predictable workflows)

---

## üí° Best Practices for Users (Already Available)

### 1. Use Parallel Patterns
```typescript
// Good: Parallel agents
const results = await runParallel([agent1, agent2, agent3], input);

// Bad: Sequential
const r1 = await run(agent1, input);
const r2 = await run(agent2, input);
const r3 = await run(agent3, input);
```

### 2. Enable Streaming for Better UX
```typescript
// Better perceived latency
const stream = await runStream(agent, input);
for await (const chunk of stream.textStream) {
  console.log(chunk); // User sees progress immediately
}
```

### 3. Use Sessions for Context Caching
```typescript
// Reuse session to avoid re-sending full history
const session = sessionManager.getSession('user-123');
await run(agent, 'Follow-up question', { session });
```

### 4. Enable RAG Embedding Caching
```typescript
// Automatic caching for repeated queries
const tool = createPineconeSearchTool(index, embeddingModel, {
  enableCache: true // Default: true
});
```

### 5. Use Race Agents for Fallbacks
```typescript
// Get fastest response
const result = await raceAgents([
  slowButAccurateAgent,
  fastButSimpleAgent
], input);
```

---

## üîç Monitoring & Debugging

### Track Latency Metrics

```typescript
// Already available via Langfuse tracing
import { initializeLangfuse } from 'tawk-agents-sdk';

initializeLangfuse({
  publicKey: 'pk-...',
  secretKey: 'sk-...'
});

// Automatic metrics:
// - Total latency
// - Tool execution time (per tool)
// - Agent execution time (per agent)
// - Model response time
// - Token usage
```

### Identify Bottlenecks

```typescript
// Check agent metrics
const result = await run(agent, input);

console.log('Total duration:', result.metadata.totalDuration);
console.log('Step count:', result.metadata.stepCount);

result.metadata.agentMetrics?.forEach(metric => {
  console.log(`${metric.agentName}:`, metric.duration, 'ms');
});
```

---

## ‚úÖ Final Recommendations

### For Immediate Use (No Code Changes)
1. ‚úÖ Use parallel patterns (`runParallel`, parallel tool calls)
2. ‚úÖ Enable streaming for better perceived latency
3. ‚úÖ Reuse sessions for context caching
4. ‚úÖ Use race agents for fallback/redundancy

### For 2026 Q1 (Low-Hanging Fruit)
1. ‚úÖ Add HTTP/2 keep-alive configuration
2. ‚úÖ Implement instruction template caching
3. ‚úÖ Add lazy loading for tools

### For 2026 Q2-Q4 (High-Impact)
1. ‚úÖ Model response caching (biggest win)
2. ‚úÖ MCP connection pooling
3. ‚úÖ Batch API calls
4. ‚úÖ Tool result streaming
5. ‚úÖ Prefetching (advanced)

---

## üìä Expected Latency Improvements

| Scenario | Current | With Phase 1 | With Phase 2 | With Phase 3 |
|----------|---------|-------------|-------------|-------------|
| **Single Agent, No Tools** | 1000ms | 950ms (-5%) | 600ms (-40%) | 550ms (-45%) |
| **Single Agent, 3 Tools** | 2500ms | 2400ms (-4%) | 1500ms (-40%) | 1200ms (-52%) |
| **Multi-Agent (3 agents)** | 3000ms | 2850ms (-5%) | 1800ms (-40%) | 1500ms (-50%) |
| **RAG Query** | 2000ms | 1900ms (-5%) | 1200ms (-40%) | 1000ms (-50%) |

**Note**: Percentages assume cache hits and optimal conditions. Real-world results will vary.

---

## üéØ Conclusion

The SDK **already has excellent latency optimizations** (‚≠ê‚≠ê‚≠ê‚≠ê 4/5), especially:
- ‚úÖ Parallel tool execution
- ‚úÖ MCP and embedding caching
- ‚úÖ Parallel agent coordination
- ‚úÖ Efficient streaming

**Recommended Next Steps**:
1. **Immediate**: Use existing parallel patterns (no code changes)
2. **Q1 2026**: Quick wins (HTTP/2, template caching)
3. **Q2 2026**: Model response caching (biggest impact)
4. **Q3-Q4 2026**: Advanced features (streaming, prefetching)

With Phase 2 optimizations, you can expect **30-50% latency reduction** for typical workflows.

---

**Assessment Date**: December 1, 2025  
**Next Review**: After Phase 1 implementation (Q1 2026)  
**Current Status**: ‚úÖ Production-ready with good latency performance

